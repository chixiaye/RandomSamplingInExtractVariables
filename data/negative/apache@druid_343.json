{
  "id" : 343,
  "expression" : "segmentsPerInterval.size()",
  "projectName" : "apache@druid",
  "commitID" : "2df42143aec6c50e9ac31d89cd75749d10d37a3d",
  "filePath" : "server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentPublisherHelper.java",
  "occurrences" : 2,
  "isArithmeticExpression" : 0,
  "isGetTypeMethod" : 0,
  "expressionList" : [ {
    "nodeContext" : "segmentsPerInterval.size()",
    "nodeType" : "MethodInvocation",
    "nodePosition" : {
      "charLength" : 26,
      "startLineNumber" : 76,
      "startColumnNumber" : 49,
      "endLineNumber" : 76,
      "endColumnNumber" : 75
    },
    "astNodeNumber" : 3,
    "astHeight" : 2,
    "parentDataList" : [ {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.Assignment,rightHandSide]",
      "nodePosition" : {
        "charLength" : 55,
        "startLineNumber" : 76,
        "startColumnNumber" : 21,
        "endLineNumber" : 76,
        "endColumnNumber" : 76
      },
      "nodeContext" : "annotateAtomicUpdateGroupFn(segmentsPerInterval.size())",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 5,
      "astHeight" : 3
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
      "nodePosition" : {
        "charLength" : 68,
        "startLineNumber" : 76,
        "startColumnNumber" : 8,
        "endLineNumber" : 76,
        "endColumnNumber" : 76
      },
      "nodeContext" : "annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size())",
      "nodeType" : "Assignment",
      "astNodeNumber" : 7,
      "astHeight" : 4
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 69,
        "startLineNumber" : 76,
        "startColumnNumber" : 8,
        "endLineNumber" : 76,
        "endColumnNumber" : 77
      },
      "nodeContext" : "annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n",
      "nodeType" : "ExpressionStatement",
      "astNodeNumber" : 8,
      "astHeight" : 5
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
      "nodePosition" : {
        "charLength" : 87,
        "startLineNumber" : 75,
        "startColumnNumber" : 56,
        "endLineNumber" : 77,
        "endColumnNumber" : 7
      },
      "nodeContext" : "{\n  annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 9,
      "astHeight" : 6
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 1660,
        "startLineNumber" : 75,
        "startColumnNumber" : 6,
        "endLineNumber" : 105,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (firstShardSpec instanceof OverwriteShardSpec) {\n  annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n}\n else if (firstShardSpec instanceof BuildingShardSpec) {\n  int expectedCorePartitionSetSize=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n  if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n}\n else if (firstShardSpec instanceof BucketNumberedShardSpec) {\n  throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n}\n else {\n  annotateFn=null;\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 96,
      "astHeight" : 13
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.EnhancedForStatement,body]",
      "nodePosition" : {
        "charLength" : 2448,
        "startLineNumber" : 60,
        "startColumnNumber" : 83,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "{\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 194,
      "astHeight" : 14
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 2527,
        "startLineNumber" : 60,
        "startColumnNumber" : 4,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "for (Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "EnhancedForStatement",
      "astNodeNumber" : 210,
      "astHeight" : 15
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodDeclaration,body]",
      "nodePosition" : {
        "charLength" : 2868,
        "startLineNumber" : 54,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "{\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 268,
      "astHeight" : 16
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.TypeDeclaration,bodyDeclarations]",
      "nodePosition" : {
        "charLength" : 3415,
        "startLineNumber" : 45,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "/** \n * This method fills missing information in the shard spec if necessary when publishing segments. - When time chunk lock is used, the non-appending task should set the proper size of the core partitions for dynamically-partitioned segments. See  {@link #annotateCorePartitionSetSizeFn}. - When segment lock is used, the overwriting task should set the proper size of the atomic update group. See  {@link #annotateAtomicUpdateGroupFn}.\n */\nstatic Set<DataSegment> annotateShardSpec(Set<DataSegment> segments){\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "MethodDeclaration",
      "astNodeNumber" : 284,
      "astHeight" : 17
    } ],
    "currentLineData" : {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 69,
        "startLineNumber" : 76,
        "startColumnNumber" : 8,
        "endLineNumber" : 76,
        "endColumnNumber" : 77
      },
      "nodeContext" : "annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n",
      "nodeType" : "ExpressionStatement",
      "astNodeNumber" : 8,
      "astHeight" : 5
    },
    "tokenLength" : 2,
    "type" : "int"
  }, {
    "nodeContext" : "segmentsPerInterval.size()",
    "nodeType" : "MethodInvocation",
    "nodePosition" : {
      "charLength" : 26,
      "startLineNumber" : 82,
      "startColumnNumber" : 43,
      "endLineNumber" : 82,
      "endColumnNumber" : 69
    },
    "astNodeNumber" : 3,
    "astHeight" : 2,
    "parentDataList" : [ {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.VariableDeclarationStatement,fragments]",
      "nodePosition" : {
        "charLength" : 57,
        "startLineNumber" : 82,
        "startColumnNumber" : 12,
        "endLineNumber" : 82,
        "endColumnNumber" : 69
      },
      "nodeContext" : "expectedCorePartitionSetSize=segmentsPerInterval.size()",
      "nodeType" : "VariableDeclarationFragment",
      "astNodeNumber" : 5,
      "astHeight" : 3
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 62,
        "startLineNumber" : 82,
        "startColumnNumber" : 8,
        "endLineNumber" : 82,
        "endColumnNumber" : 70
      },
      "nodeContext" : "int expectedCorePartitionSetSize=segmentsPerInterval.size();\n",
      "nodeType" : "VariableDeclarationStatement",
      "astNodeNumber" : 7,
      "astHeight" : 4
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
      "nodePosition" : {
        "charLength" : 1271,
        "startLineNumber" : 77,
        "startColumnNumber" : 62,
        "endLineNumber" : 101,
        "endColumnNumber" : 7
      },
      "nodeContext" : "{\n  int expectedCorePartitionSetSize=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n  if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 60,
      "astHeight" : 11
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,elseStatement]",
      "nodePosition" : {
        "charLength" : 1517,
        "startLineNumber" : 77,
        "startColumnNumber" : 13,
        "endLineNumber" : 105,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (firstShardSpec instanceof BuildingShardSpec) {\n  int expectedCorePartitionSetSize=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n  if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n}\n else if (firstShardSpec instanceof BucketNumberedShardSpec) {\n  throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n}\n else {\n  annotateFn=null;\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 82,
      "astHeight" : 12
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 1660,
        "startLineNumber" : 75,
        "startColumnNumber" : 6,
        "endLineNumber" : 105,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (firstShardSpec instanceof OverwriteShardSpec) {\n  annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n}\n else if (firstShardSpec instanceof BuildingShardSpec) {\n  int expectedCorePartitionSetSize=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n  if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n}\n else if (firstShardSpec instanceof BucketNumberedShardSpec) {\n  throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n}\n else {\n  annotateFn=null;\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 96,
      "astHeight" : 13
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.EnhancedForStatement,body]",
      "nodePosition" : {
        "charLength" : 2448,
        "startLineNumber" : 60,
        "startColumnNumber" : 83,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "{\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 194,
      "astHeight" : 14
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 2527,
        "startLineNumber" : 60,
        "startColumnNumber" : 4,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "for (Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "EnhancedForStatement",
      "astNodeNumber" : 210,
      "astHeight" : 15
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodDeclaration,body]",
      "nodePosition" : {
        "charLength" : 2868,
        "startLineNumber" : 54,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "{\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 268,
      "astHeight" : 16
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.TypeDeclaration,bodyDeclarations]",
      "nodePosition" : {
        "charLength" : 3415,
        "startLineNumber" : 45,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "/** \n * This method fills missing information in the shard spec if necessary when publishing segments. - When time chunk lock is used, the non-appending task should set the proper size of the core partitions for dynamically-partitioned segments. See  {@link #annotateCorePartitionSetSizeFn}. - When segment lock is used, the overwriting task should set the proper size of the atomic update group. See  {@link #annotateAtomicUpdateGroupFn}.\n */\nstatic Set<DataSegment> annotateShardSpec(Set<DataSegment> segments){\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "MethodDeclaration",
      "astNodeNumber" : 284,
      "astHeight" : 17
    } ],
    "currentLineData" : {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 62,
        "startLineNumber" : 82,
        "startColumnNumber" : 8,
        "endLineNumber" : 82,
        "endColumnNumber" : 70
      },
      "nodeContext" : "int expectedCorePartitionSetSize=segmentsPerInterval.size();\n",
      "nodeType" : "VariableDeclarationStatement",
      "astNodeNumber" : 7,
      "astHeight" : 4
    },
    "tokenLength" : 2,
    "type" : "int"
  } ],
  "positionList" : [ {
    "charLength" : 26,
    "startLineNumber" : 76,
    "startColumnNumber" : 49,
    "endLineNumber" : 76,
    "endColumnNumber" : 75
  }, {
    "charLength" : 26,
    "startLineNumber" : 82,
    "startColumnNumber" : 43,
    "endLineNumber" : 82,
    "endColumnNumber" : 69
  } ],
  "layoutRelationDataList" : [ {
    "firstKey" : 0,
    "secondKey" : 1,
    "layout" : 4
  }, {
    "firstKey" : 1,
    "secondKey" : 0,
    "layout" : 4
  } ]
}