{
  "id" : 347,
  "expression" : "segmentsPerInterval",
  "projectName" : "apache@druid",
  "commitID" : "2df42143aec6c50e9ac31d89cd75749d10d37a3d",
  "filePath" : "server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentPublisherHelper.java",
  "occurrences" : 8,
  "isArithmeticExpression" : 0,
  "isGetTypeMethod" : 0,
  "expressionList" : [ {
    "nodeContext" : "segmentsPerInterval",
    "nodeType" : "SimpleName",
    "nodePosition" : {
      "charLength" : 19,
      "startLineNumber" : 63,
      "startColumnNumber" : 39,
      "endLineNumber" : 63,
      "endColumnNumber" : 58
    },
    "astNodeNumber" : 1,
    "astHeight" : 1,
    "parentDataList" : [ {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
      "nodePosition" : {
        "charLength" : 26,
        "startLineNumber" : 63,
        "startColumnNumber" : 39,
        "endLineNumber" : 63,
        "endColumnNumber" : 65
      },
      "nodeContext" : "segmentsPerInterval.get(0)",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 4,
      "astHeight" : 2
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.VariableDeclarationFragment,initializer]",
      "nodePosition" : {
        "charLength" : 41,
        "startLineNumber" : 63,
        "startColumnNumber" : 39,
        "endLineNumber" : 63,
        "endColumnNumber" : 80
      },
      "nodeContext" : "segmentsPerInterval.get(0).getShardSpec()",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 6,
      "astHeight" : 3
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.VariableDeclarationStatement,fragments]",
      "nodePosition" : {
        "charLength" : 58,
        "startLineNumber" : 63,
        "startColumnNumber" : 22,
        "endLineNumber" : 63,
        "endColumnNumber" : 80
      },
      "nodeContext" : "firstShardSpec=segmentsPerInterval.get(0).getShardSpec()",
      "nodeType" : "VariableDeclarationFragment",
      "astNodeNumber" : 8,
      "astHeight" : 4
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 75,
        "startLineNumber" : 63,
        "startColumnNumber" : 6,
        "endLineNumber" : 63,
        "endColumnNumber" : 81
      },
      "nodeContext" : "final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n",
      "nodeType" : "VariableDeclarationStatement",
      "astNodeNumber" : 12,
      "astHeight" : 5
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.EnhancedForStatement,body]",
      "nodePosition" : {
        "charLength" : 2448,
        "startLineNumber" : 60,
        "startColumnNumber" : 83,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "{\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 194,
      "astHeight" : 14
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 2527,
        "startLineNumber" : 60,
        "startColumnNumber" : 4,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "for (Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "EnhancedForStatement",
      "astNodeNumber" : 210,
      "astHeight" : 15
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodDeclaration,body]",
      "nodePosition" : {
        "charLength" : 2868,
        "startLineNumber" : 54,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "{\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 268,
      "astHeight" : 16
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.TypeDeclaration,bodyDeclarations]",
      "nodePosition" : {
        "charLength" : 3415,
        "startLineNumber" : 45,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "/** \n * This method fills missing information in the shard spec if necessary when publishing segments. - When time chunk lock is used, the non-appending task should set the proper size of the core partitions for dynamically-partitioned segments. See  {@link #annotateCorePartitionSetSizeFn}. - When segment lock is used, the overwriting task should set the proper size of the atomic update group. See  {@link #annotateAtomicUpdateGroupFn}.\n */\nstatic Set<DataSegment> annotateShardSpec(Set<DataSegment> segments){\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "MethodDeclaration",
      "astNodeNumber" : 284,
      "astHeight" : 17
    } ],
    "currentLineData" : {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 75,
        "startLineNumber" : 63,
        "startColumnNumber" : 6,
        "endLineNumber" : 63,
        "endColumnNumber" : 81
      },
      "nodeContext" : "final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n",
      "nodeType" : "VariableDeclarationStatement",
      "astNodeNumber" : 12,
      "astHeight" : 5
    },
    "tokenLength" : 1,
    "type" : "java.util.List<org.apache.druid.timeline.DataSegment>"
  }, {
    "nodeContext" : "segmentsPerInterval",
    "nodeType" : "SimpleName",
    "nodePosition" : {
      "charLength" : 19,
      "startLineNumber" : 64,
      "startColumnNumber" : 34,
      "endLineNumber" : 64,
      "endColumnNumber" : 53
    },
    "astNodeNumber" : 1,
    "astHeight" : 1,
    "parentDataList" : [ {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
      "nodePosition" : {
        "charLength" : 28,
        "startLineNumber" : 64,
        "startColumnNumber" : 34,
        "endLineNumber" : 64,
        "endColumnNumber" : 62
      },
      "nodeContext" : "segmentsPerInterval.stream()",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 3,
      "astHeight" : 2
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.VariableDeclarationFragment,initializer]",
      "nodePosition" : {
        "charLength" : 130,
        "startLineNumber" : 64,
        "startColumnNumber" : 34,
        "endLineNumber" : 66,
        "endColumnNumber" : 7
      },
      "nodeContext" : "segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass())",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 17,
      "astHeight" : 6
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.VariableDeclarationStatement,fragments]",
      "nodePosition" : {
        "charLength" : 144,
        "startLineNumber" : 64,
        "startColumnNumber" : 20,
        "endLineNumber" : 66,
        "endColumnNumber" : 7
      },
      "nodeContext" : "anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass())",
      "nodeType" : "VariableDeclarationFragment",
      "astNodeNumber" : 19,
      "astHeight" : 7
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 159,
        "startLineNumber" : 64,
        "startColumnNumber" : 6,
        "endLineNumber" : 66,
        "endColumnNumber" : 8
      },
      "nodeContext" : "final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n",
      "nodeType" : "VariableDeclarationStatement",
      "astNodeNumber" : 22,
      "astHeight" : 8
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.EnhancedForStatement,body]",
      "nodePosition" : {
        "charLength" : 2448,
        "startLineNumber" : 60,
        "startColumnNumber" : 83,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "{\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 194,
      "astHeight" : 14
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 2527,
        "startLineNumber" : 60,
        "startColumnNumber" : 4,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "for (Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "EnhancedForStatement",
      "astNodeNumber" : 210,
      "astHeight" : 15
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodDeclaration,body]",
      "nodePosition" : {
        "charLength" : 2868,
        "startLineNumber" : 54,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "{\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 268,
      "astHeight" : 16
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.TypeDeclaration,bodyDeclarations]",
      "nodePosition" : {
        "charLength" : 3415,
        "startLineNumber" : 45,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "/** \n * This method fills missing information in the shard spec if necessary when publishing segments. - When time chunk lock is used, the non-appending task should set the proper size of the core partitions for dynamically-partitioned segments. See  {@link #annotateCorePartitionSetSizeFn}. - When segment lock is used, the overwriting task should set the proper size of the atomic update group. See  {@link #annotateAtomicUpdateGroupFn}.\n */\nstatic Set<DataSegment> annotateShardSpec(Set<DataSegment> segments){\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "MethodDeclaration",
      "astNodeNumber" : 284,
      "astHeight" : 17
    } ],
    "currentLineData" : {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
      "nodePosition" : {
        "charLength" : 28,
        "startLineNumber" : 64,
        "startColumnNumber" : 34,
        "endLineNumber" : 64,
        "endColumnNumber" : 62
      },
      "nodeContext" : "segmentsPerInterval.stream()",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 3,
      "astHeight" : 2
    },
    "tokenLength" : 1,
    "type" : "java.util.List<org.apache.druid.timeline.DataSegment>"
  }, {
    "nodeContext" : "segmentsPerInterval",
    "nodeType" : "SimpleName",
    "nodePosition" : {
      "charLength" : 19,
      "startLineNumber" : 71,
      "startColumnNumber" : 12,
      "endLineNumber" : 71,
      "endColumnNumber" : 31
    },
    "astNodeNumber" : 1,
    "astHeight" : 1,
    "parentDataList" : [ {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.ThrowStatement,expression]",
      "nodePosition" : {
        "charLength" : 142,
        "startLineNumber" : 68,
        "startColumnNumber" : 14,
        "endLineNumber" : 72,
        "endColumnNumber" : 9
      },
      "nodeContext" : "new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval)",
      "nodeType" : "ClassInstanceCreation",
      "astNodeNumber" : 6,
      "astHeight" : 3
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 149,
        "startLineNumber" : 68,
        "startColumnNumber" : 8,
        "endLineNumber" : 72,
        "endColumnNumber" : 10
      },
      "nodeContext" : "throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n",
      "nodeType" : "ThrowStatement",
      "astNodeNumber" : 7,
      "astHeight" : 4
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
      "nodePosition" : {
        "charLength" : 167,
        "startLineNumber" : 67,
        "startColumnNumber" : 23,
        "endLineNumber" : 73,
        "endColumnNumber" : 7
      },
      "nodeContext" : "{\n  throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 8,
      "astHeight" : 5
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 184,
        "startLineNumber" : 67,
        "startColumnNumber" : 6,
        "endLineNumber" : 73,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (anyMismatch) {\n  throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 10,
      "astHeight" : 6
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.EnhancedForStatement,body]",
      "nodePosition" : {
        "charLength" : 2448,
        "startLineNumber" : 60,
        "startColumnNumber" : 83,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "{\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 194,
      "astHeight" : 14
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 2527,
        "startLineNumber" : 60,
        "startColumnNumber" : 4,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "for (Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "EnhancedForStatement",
      "astNodeNumber" : 210,
      "astHeight" : 15
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodDeclaration,body]",
      "nodePosition" : {
        "charLength" : 2868,
        "startLineNumber" : 54,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "{\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 268,
      "astHeight" : 16
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.TypeDeclaration,bodyDeclarations]",
      "nodePosition" : {
        "charLength" : 3415,
        "startLineNumber" : 45,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "/** \n * This method fills missing information in the shard spec if necessary when publishing segments. - When time chunk lock is used, the non-appending task should set the proper size of the core partitions for dynamically-partitioned segments. See  {@link #annotateCorePartitionSetSizeFn}. - When segment lock is used, the overwriting task should set the proper size of the atomic update group. See  {@link #annotateAtomicUpdateGroupFn}.\n */\nstatic Set<DataSegment> annotateShardSpec(Set<DataSegment> segments){\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "MethodDeclaration",
      "astNodeNumber" : 284,
      "astHeight" : 17
    } ],
    "currentLineData" : {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.ThrowStatement,expression]",
      "nodePosition" : {
        "charLength" : 142,
        "startLineNumber" : 68,
        "startColumnNumber" : 14,
        "endLineNumber" : 72,
        "endColumnNumber" : 9
      },
      "nodeContext" : "new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval)",
      "nodeType" : "ClassInstanceCreation",
      "astNodeNumber" : 6,
      "astHeight" : 3
    },
    "tokenLength" : 1,
    "type" : "java.util.List<org.apache.druid.timeline.DataSegment>"
  }, {
    "nodeContext" : "segmentsPerInterval",
    "nodeType" : "SimpleName",
    "nodePosition" : {
      "charLength" : 19,
      "startLineNumber" : 76,
      "startColumnNumber" : 49,
      "endLineNumber" : 76,
      "endColumnNumber" : 68
    },
    "astNodeNumber" : 1,
    "astHeight" : 1,
    "parentDataList" : [ {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
      "nodePosition" : {
        "charLength" : 26,
        "startLineNumber" : 76,
        "startColumnNumber" : 49,
        "endLineNumber" : 76,
        "endColumnNumber" : 75
      },
      "nodeContext" : "segmentsPerInterval.size()",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 3,
      "astHeight" : 2
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.Assignment,rightHandSide]",
      "nodePosition" : {
        "charLength" : 55,
        "startLineNumber" : 76,
        "startColumnNumber" : 21,
        "endLineNumber" : 76,
        "endColumnNumber" : 76
      },
      "nodeContext" : "annotateAtomicUpdateGroupFn(segmentsPerInterval.size())",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 5,
      "astHeight" : 3
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
      "nodePosition" : {
        "charLength" : 68,
        "startLineNumber" : 76,
        "startColumnNumber" : 8,
        "endLineNumber" : 76,
        "endColumnNumber" : 76
      },
      "nodeContext" : "annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size())",
      "nodeType" : "Assignment",
      "astNodeNumber" : 7,
      "astHeight" : 4
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 69,
        "startLineNumber" : 76,
        "startColumnNumber" : 8,
        "endLineNumber" : 76,
        "endColumnNumber" : 77
      },
      "nodeContext" : "annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n",
      "nodeType" : "ExpressionStatement",
      "astNodeNumber" : 8,
      "astHeight" : 5
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
      "nodePosition" : {
        "charLength" : 87,
        "startLineNumber" : 75,
        "startColumnNumber" : 56,
        "endLineNumber" : 77,
        "endColumnNumber" : 7
      },
      "nodeContext" : "{\n  annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 9,
      "astHeight" : 6
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 1660,
        "startLineNumber" : 75,
        "startColumnNumber" : 6,
        "endLineNumber" : 105,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (firstShardSpec instanceof OverwriteShardSpec) {\n  annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n}\n else if (firstShardSpec instanceof BuildingShardSpec) {\n  int expectedCorePartitionSetSize=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n  if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n}\n else if (firstShardSpec instanceof BucketNumberedShardSpec) {\n  throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n}\n else {\n  annotateFn=null;\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 96,
      "astHeight" : 13
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.EnhancedForStatement,body]",
      "nodePosition" : {
        "charLength" : 2448,
        "startLineNumber" : 60,
        "startColumnNumber" : 83,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "{\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 194,
      "astHeight" : 14
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 2527,
        "startLineNumber" : 60,
        "startColumnNumber" : 4,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "for (Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "EnhancedForStatement",
      "astNodeNumber" : 210,
      "astHeight" : 15
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodDeclaration,body]",
      "nodePosition" : {
        "charLength" : 2868,
        "startLineNumber" : 54,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "{\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 268,
      "astHeight" : 16
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.TypeDeclaration,bodyDeclarations]",
      "nodePosition" : {
        "charLength" : 3415,
        "startLineNumber" : 45,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "/** \n * This method fills missing information in the shard spec if necessary when publishing segments. - When time chunk lock is used, the non-appending task should set the proper size of the core partitions for dynamically-partitioned segments. See  {@link #annotateCorePartitionSetSizeFn}. - When segment lock is used, the overwriting task should set the proper size of the atomic update group. See  {@link #annotateAtomicUpdateGroupFn}.\n */\nstatic Set<DataSegment> annotateShardSpec(Set<DataSegment> segments){\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "MethodDeclaration",
      "astNodeNumber" : 284,
      "astHeight" : 17
    } ],
    "currentLineData" : {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 69,
        "startLineNumber" : 76,
        "startColumnNumber" : 8,
        "endLineNumber" : 76,
        "endColumnNumber" : 77
      },
      "nodeContext" : "annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n",
      "nodeType" : "ExpressionStatement",
      "astNodeNumber" : 8,
      "astHeight" : 5
    },
    "tokenLength" : 1,
    "type" : "java.util.List<org.apache.druid.timeline.DataSegment>"
  }, {
    "nodeContext" : "segmentsPerInterval",
    "nodeType" : "SimpleName",
    "nodePosition" : {
      "charLength" : 19,
      "startLineNumber" : 82,
      "startColumnNumber" : 43,
      "endLineNumber" : 82,
      "endColumnNumber" : 62
    },
    "astNodeNumber" : 1,
    "astHeight" : 1,
    "parentDataList" : [ {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.VariableDeclarationFragment,initializer]",
      "nodePosition" : {
        "charLength" : 26,
        "startLineNumber" : 82,
        "startColumnNumber" : 43,
        "endLineNumber" : 82,
        "endColumnNumber" : 69
      },
      "nodeContext" : "segmentsPerInterval.size()",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 3,
      "astHeight" : 2
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.VariableDeclarationStatement,fragments]",
      "nodePosition" : {
        "charLength" : 57,
        "startLineNumber" : 82,
        "startColumnNumber" : 12,
        "endLineNumber" : 82,
        "endColumnNumber" : 69
      },
      "nodeContext" : "expectedCorePartitionSetSize=segmentsPerInterval.size()",
      "nodeType" : "VariableDeclarationFragment",
      "astNodeNumber" : 5,
      "astHeight" : 3
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 62,
        "startLineNumber" : 82,
        "startColumnNumber" : 8,
        "endLineNumber" : 82,
        "endColumnNumber" : 70
      },
      "nodeContext" : "int expectedCorePartitionSetSize=segmentsPerInterval.size();\n",
      "nodeType" : "VariableDeclarationStatement",
      "astNodeNumber" : 7,
      "astHeight" : 4
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
      "nodePosition" : {
        "charLength" : 1271,
        "startLineNumber" : 77,
        "startColumnNumber" : 62,
        "endLineNumber" : 101,
        "endColumnNumber" : 7
      },
      "nodeContext" : "{\n  int expectedCorePartitionSetSize=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n  if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 60,
      "astHeight" : 11
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,elseStatement]",
      "nodePosition" : {
        "charLength" : 1517,
        "startLineNumber" : 77,
        "startColumnNumber" : 13,
        "endLineNumber" : 105,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (firstShardSpec instanceof BuildingShardSpec) {\n  int expectedCorePartitionSetSize=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n  if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n}\n else if (firstShardSpec instanceof BucketNumberedShardSpec) {\n  throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n}\n else {\n  annotateFn=null;\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 82,
      "astHeight" : 12
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 1660,
        "startLineNumber" : 75,
        "startColumnNumber" : 6,
        "endLineNumber" : 105,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (firstShardSpec instanceof OverwriteShardSpec) {\n  annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n}\n else if (firstShardSpec instanceof BuildingShardSpec) {\n  int expectedCorePartitionSetSize=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n  if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n}\n else if (firstShardSpec instanceof BucketNumberedShardSpec) {\n  throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n}\n else {\n  annotateFn=null;\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 96,
      "astHeight" : 13
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.EnhancedForStatement,body]",
      "nodePosition" : {
        "charLength" : 2448,
        "startLineNumber" : 60,
        "startColumnNumber" : 83,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "{\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 194,
      "astHeight" : 14
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 2527,
        "startLineNumber" : 60,
        "startColumnNumber" : 4,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "for (Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "EnhancedForStatement",
      "astNodeNumber" : 210,
      "astHeight" : 15
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodDeclaration,body]",
      "nodePosition" : {
        "charLength" : 2868,
        "startLineNumber" : 54,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "{\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 268,
      "astHeight" : 16
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.TypeDeclaration,bodyDeclarations]",
      "nodePosition" : {
        "charLength" : 3415,
        "startLineNumber" : 45,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "/** \n * This method fills missing information in the shard spec if necessary when publishing segments. - When time chunk lock is used, the non-appending task should set the proper size of the core partitions for dynamically-partitioned segments. See  {@link #annotateCorePartitionSetSizeFn}. - When segment lock is used, the overwriting task should set the proper size of the atomic update group. See  {@link #annotateAtomicUpdateGroupFn}.\n */\nstatic Set<DataSegment> annotateShardSpec(Set<DataSegment> segments){\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "MethodDeclaration",
      "astNodeNumber" : 284,
      "astHeight" : 17
    } ],
    "currentLineData" : {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 62,
        "startLineNumber" : 82,
        "startColumnNumber" : 8,
        "endLineNumber" : 82,
        "endColumnNumber" : 70
      },
      "nodeContext" : "int expectedCorePartitionSetSize=segmentsPerInterval.size();\n",
      "nodeType" : "VariableDeclarationStatement",
      "astNodeNumber" : 7,
      "astHeight" : 4
    },
    "tokenLength" : 1,
    "type" : "java.util.List<org.apache.druid.timeline.DataSegment>"
  }, {
    "nodeContext" : "segmentsPerInterval",
    "nodeType" : "SimpleName",
    "nodePosition" : {
      "charLength" : 19,
      "startLineNumber" : 84,
      "startColumnNumber" : 12,
      "endLineNumber" : 84,
      "endColumnNumber" : 31
    },
    "astNodeNumber" : 1,
    "astHeight" : 1,
    "parentDataList" : [ {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
      "nodePosition" : {
        "charLength" : 45,
        "startLineNumber" : 84,
        "startColumnNumber" : 12,
        "endLineNumber" : 85,
        "endColumnNumber" : 25
      },
      "nodeContext" : "segmentsPerInterval.stream()",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 3,
      "astHeight" : 2
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
      "nodePosition" : {
        "charLength" : 153,
        "startLineNumber" : 84,
        "startColumnNumber" : 12,
        "endLineNumber" : 86,
        "endColumnNumber" : 107
      },
      "nodeContext" : "segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize)",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 15,
      "astHeight" : 6
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
      "nodePosition" : {
        "charLength" : 178,
        "startLineNumber" : 84,
        "startColumnNumber" : 12,
        "endLineNumber" : 87,
        "endColumnNumber" : 24
      },
      "nodeContext" : "segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count()",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 17,
      "astHeight" : 7
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.VariableDeclarationFragment,initializer]",
      "nodePosition" : {
        "charLength" : 217,
        "startLineNumber" : 83,
        "startColumnNumber" : 41,
        "endLineNumber" : 88,
        "endColumnNumber" : 9
      },
      "nodeContext" : "Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count())",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 20,
      "astHeight" : 8
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.VariableDeclarationStatement,fragments]",
      "nodePosition" : {
        "charLength" : 246,
        "startLineNumber" : 83,
        "startColumnNumber" : 12,
        "endLineNumber" : 88,
        "endColumnNumber" : 9
      },
      "nodeContext" : "actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count())",
      "nodeType" : "VariableDeclarationFragment",
      "astNodeNumber" : 22,
      "astHeight" : 9
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 251,
        "startLineNumber" : 83,
        "startColumnNumber" : 8,
        "endLineNumber" : 88,
        "endColumnNumber" : 10
      },
      "nodeContext" : "int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n",
      "nodeType" : "VariableDeclarationStatement",
      "astNodeNumber" : 24,
      "astHeight" : 10
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
      "nodePosition" : {
        "charLength" : 1271,
        "startLineNumber" : 77,
        "startColumnNumber" : 62,
        "endLineNumber" : 101,
        "endColumnNumber" : 7
      },
      "nodeContext" : "{\n  int expectedCorePartitionSetSize=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n  if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 60,
      "astHeight" : 11
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,elseStatement]",
      "nodePosition" : {
        "charLength" : 1517,
        "startLineNumber" : 77,
        "startColumnNumber" : 13,
        "endLineNumber" : 105,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (firstShardSpec instanceof BuildingShardSpec) {\n  int expectedCorePartitionSetSize=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n  if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n}\n else if (firstShardSpec instanceof BucketNumberedShardSpec) {\n  throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n}\n else {\n  annotateFn=null;\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 82,
      "astHeight" : 12
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 1660,
        "startLineNumber" : 75,
        "startColumnNumber" : 6,
        "endLineNumber" : 105,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (firstShardSpec instanceof OverwriteShardSpec) {\n  annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n}\n else if (firstShardSpec instanceof BuildingShardSpec) {\n  int expectedCorePartitionSetSize=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n  if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n}\n else if (firstShardSpec instanceof BucketNumberedShardSpec) {\n  throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n}\n else {\n  annotateFn=null;\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 96,
      "astHeight" : 13
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.EnhancedForStatement,body]",
      "nodePosition" : {
        "charLength" : 2448,
        "startLineNumber" : 60,
        "startColumnNumber" : 83,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "{\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 194,
      "astHeight" : 14
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 2527,
        "startLineNumber" : 60,
        "startColumnNumber" : 4,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "for (Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "EnhancedForStatement",
      "astNodeNumber" : 210,
      "astHeight" : 15
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodDeclaration,body]",
      "nodePosition" : {
        "charLength" : 2868,
        "startLineNumber" : 54,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "{\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 268,
      "astHeight" : 16
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.TypeDeclaration,bodyDeclarations]",
      "nodePosition" : {
        "charLength" : 3415,
        "startLineNumber" : 45,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "/** \n * This method fills missing information in the shard spec if necessary when publishing segments. - When time chunk lock is used, the non-appending task should set the proper size of the core partitions for dynamically-partitioned segments. See  {@link #annotateCorePartitionSetSizeFn}. - When segment lock is used, the overwriting task should set the proper size of the atomic update group. See  {@link #annotateAtomicUpdateGroupFn}.\n */\nstatic Set<DataSegment> annotateShardSpec(Set<DataSegment> segments){\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "MethodDeclaration",
      "astNodeNumber" : 284,
      "astHeight" : 17
    } ],
    "currentLineData" : {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
      "nodePosition" : {
        "charLength" : 45,
        "startLineNumber" : 84,
        "startColumnNumber" : 12,
        "endLineNumber" : 85,
        "endColumnNumber" : 25
      },
      "nodeContext" : "segmentsPerInterval.stream()",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 3,
      "astHeight" : 2
    },
    "tokenLength" : 1,
    "type" : "java.util.List<org.apache.druid.timeline.DataSegment>"
  }, {
    "nodeContext" : "segmentsPerInterval",
    "nodeType" : "SimpleName",
    "nodePosition" : {
      "charLength" : 19,
      "startLineNumber" : 90,
      "startColumnNumber" : 28,
      "endLineNumber" : 90,
      "endColumnNumber" : 47
    },
    "astNodeNumber" : 1,
    "astHeight" : 1,
    "parentDataList" : [ {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
      "nodePosition" : {
        "charLength" : 94,
        "startLineNumber" : 90,
        "startColumnNumber" : 10,
        "endLineNumber" : 90,
        "endColumnNumber" : 104
      },
      "nodeContext" : "LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\")",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 5,
      "astHeight" : 2
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 95,
        "startLineNumber" : 90,
        "startColumnNumber" : 10,
        "endLineNumber" : 90,
        "endColumnNumber" : 105
      },
      "nodeContext" : "LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n",
      "nodeType" : "ExpressionStatement",
      "astNodeNumber" : 6,
      "astHeight" : 3
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
      "nodePosition" : {
        "charLength" : 522,
        "startLineNumber" : 89,
        "startColumnNumber" : 72,
        "endLineNumber" : 99,
        "endColumnNumber" : 9
      },
      "nodeContext" : "{\n  LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n  throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 18,
      "astHeight" : 5
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 586,
        "startLineNumber" : 89,
        "startColumnNumber" : 8,
        "endLineNumber" : 99,
        "endColumnNumber" : 9
      },
      "nodeContext" : "if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n  LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n  throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 22,
      "astHeight" : 6
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
      "nodePosition" : {
        "charLength" : 1271,
        "startLineNumber" : 77,
        "startColumnNumber" : 62,
        "endLineNumber" : 101,
        "endColumnNumber" : 7
      },
      "nodeContext" : "{\n  int expectedCorePartitionSetSize=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n  if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 60,
      "astHeight" : 11
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,elseStatement]",
      "nodePosition" : {
        "charLength" : 1517,
        "startLineNumber" : 77,
        "startColumnNumber" : 13,
        "endLineNumber" : 105,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (firstShardSpec instanceof BuildingShardSpec) {\n  int expectedCorePartitionSetSize=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n  if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n}\n else if (firstShardSpec instanceof BucketNumberedShardSpec) {\n  throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n}\n else {\n  annotateFn=null;\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 82,
      "astHeight" : 12
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 1660,
        "startLineNumber" : 75,
        "startColumnNumber" : 6,
        "endLineNumber" : 105,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (firstShardSpec instanceof OverwriteShardSpec) {\n  annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n}\n else if (firstShardSpec instanceof BuildingShardSpec) {\n  int expectedCorePartitionSetSize=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n  if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n}\n else if (firstShardSpec instanceof BucketNumberedShardSpec) {\n  throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n}\n else {\n  annotateFn=null;\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 96,
      "astHeight" : 13
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.EnhancedForStatement,body]",
      "nodePosition" : {
        "charLength" : 2448,
        "startLineNumber" : 60,
        "startColumnNumber" : 83,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "{\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 194,
      "astHeight" : 14
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 2527,
        "startLineNumber" : 60,
        "startColumnNumber" : 4,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "for (Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "EnhancedForStatement",
      "astNodeNumber" : 210,
      "astHeight" : 15
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodDeclaration,body]",
      "nodePosition" : {
        "charLength" : 2868,
        "startLineNumber" : 54,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "{\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 268,
      "astHeight" : 16
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.TypeDeclaration,bodyDeclarations]",
      "nodePosition" : {
        "charLength" : 3415,
        "startLineNumber" : 45,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "/** \n * This method fills missing information in the shard spec if necessary when publishing segments. - When time chunk lock is used, the non-appending task should set the proper size of the core partitions for dynamically-partitioned segments. See  {@link #annotateCorePartitionSetSizeFn}. - When segment lock is used, the overwriting task should set the proper size of the atomic update group. See  {@link #annotateAtomicUpdateGroupFn}.\n */\nstatic Set<DataSegment> annotateShardSpec(Set<DataSegment> segments){\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "MethodDeclaration",
      "astNodeNumber" : 284,
      "astHeight" : 17
    } ],
    "currentLineData" : {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 95,
        "startLineNumber" : 90,
        "startColumnNumber" : 10,
        "endLineNumber" : 90,
        "endColumnNumber" : 105
      },
      "nodeContext" : "LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n",
      "nodeType" : "ExpressionStatement",
      "astNodeNumber" : 6,
      "astHeight" : 3
    },
    "tokenLength" : 1,
    "type" : "java.util.List<org.apache.druid.timeline.DataSegment>"
  }, {
    "nodeContext" : "segmentsPerInterval",
    "nodeType" : "SimpleName",
    "nodePosition" : {
      "charLength" : 19,
      "startLineNumber" : 108,
      "startColumnNumber" : 41,
      "endLineNumber" : 108,
      "endColumnNumber" : 60
    },
    "astNodeNumber" : 1,
    "astHeight" : 1,
    "parentDataList" : [ {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
      "nodePosition" : {
        "charLength" : 28,
        "startLineNumber" : 108,
        "startColumnNumber" : 41,
        "endLineNumber" : 108,
        "endColumnNumber" : 69
      },
      "nodeContext" : "segmentsPerInterval.stream()",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 3,
      "astHeight" : 2
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodInvocation,expression]",
      "nodePosition" : {
        "charLength" : 44,
        "startLineNumber" : 108,
        "startColumnNumber" : 41,
        "endLineNumber" : 108,
        "endColumnNumber" : 85
      },
      "nodeContext" : "segmentsPerInterval.stream().map(annotateFn)",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 6,
      "astHeight" : 3
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
      "nodePosition" : {
        "charLength" : 73,
        "startLineNumber" : 108,
        "startColumnNumber" : 41,
        "endLineNumber" : 108,
        "endColumnNumber" : 114
      },
      "nodeContext" : "segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList())",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 11,
      "astHeight" : 4
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
      "nodePosition" : {
        "charLength" : 107,
        "startLineNumber" : 108,
        "startColumnNumber" : 8,
        "endLineNumber" : 108,
        "endColumnNumber" : 115
      },
      "nodeContext" : "intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()))",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 15,
      "astHeight" : 5
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 108,
        "startLineNumber" : 108,
        "startColumnNumber" : 8,
        "endLineNumber" : 108,
        "endColumnNumber" : 116
      },
      "nodeContext" : "intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n",
      "nodeType" : "ExpressionStatement",
      "astNodeNumber" : 16,
      "astHeight" : 6
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
      "nodePosition" : {
        "charLength" : 126,
        "startLineNumber" : 107,
        "startColumnNumber" : 30,
        "endLineNumber" : 109,
        "endColumnNumber" : 7
      },
      "nodeContext" : "{\n  intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 17,
      "astHeight" : 7
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 150,
        "startLineNumber" : 107,
        "startColumnNumber" : 6,
        "endLineNumber" : 109,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (annotateFn != null) {\n  intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 21,
      "astHeight" : 8
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.EnhancedForStatement,body]",
      "nodePosition" : {
        "charLength" : 2448,
        "startLineNumber" : 60,
        "startColumnNumber" : 83,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "{\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 194,
      "astHeight" : 14
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 2527,
        "startLineNumber" : 60,
        "startColumnNumber" : 4,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "for (Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int expectedCorePartitionSetSize=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n    if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "EnhancedForStatement",
      "astNodeNumber" : 210,
      "astHeight" : 15
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodDeclaration,body]",
      "nodePosition" : {
        "charLength" : 2868,
        "startLineNumber" : 54,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "{\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 268,
      "astHeight" : 16
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.TypeDeclaration,bodyDeclarations]",
      "nodePosition" : {
        "charLength" : 3415,
        "startLineNumber" : 45,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "/** \n * This method fills missing information in the shard spec if necessary when publishing segments. - When time chunk lock is used, the non-appending task should set the proper size of the core partitions for dynamically-partitioned segments. See  {@link #annotateCorePartitionSetSizeFn}. - When segment lock is used, the overwriting task should set the proper size of the atomic update group. See  {@link #annotateAtomicUpdateGroupFn}.\n */\nstatic Set<DataSegment> annotateShardSpec(Set<DataSegment> segments){\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int expectedCorePartitionSetSize=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < expectedCorePartitionSetSize).count());\n      if (expectedCorePartitionSetSize != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,expectedCorePartitionSetSize,actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(expectedCorePartitionSetSize);\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "MethodDeclaration",
      "astNodeNumber" : 284,
      "astHeight" : 17
    } ],
    "currentLineData" : {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 108,
        "startLineNumber" : 108,
        "startColumnNumber" : 8,
        "endLineNumber" : 108,
        "endColumnNumber" : 116
      },
      "nodeContext" : "intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n",
      "nodeType" : "ExpressionStatement",
      "astNodeNumber" : 16,
      "astHeight" : 6
    },
    "tokenLength" : 1,
    "type" : "java.util.List<org.apache.druid.timeline.DataSegment>"
  } ],
  "positionList" : [ {
    "charLength" : 19,
    "startLineNumber" : 63,
    "startColumnNumber" : 39,
    "endLineNumber" : 63,
    "endColumnNumber" : 58
  }, {
    "charLength" : 19,
    "startLineNumber" : 64,
    "startColumnNumber" : 34,
    "endLineNumber" : 64,
    "endColumnNumber" : 53
  }, {
    "charLength" : 19,
    "startLineNumber" : 71,
    "startColumnNumber" : 12,
    "endLineNumber" : 71,
    "endColumnNumber" : 31
  }, {
    "charLength" : 19,
    "startLineNumber" : 76,
    "startColumnNumber" : 49,
    "endLineNumber" : 76,
    "endColumnNumber" : 68
  }, {
    "charLength" : 19,
    "startLineNumber" : 82,
    "startColumnNumber" : 43,
    "endLineNumber" : 82,
    "endColumnNumber" : 62
  }, {
    "charLength" : 19,
    "startLineNumber" : 84,
    "startColumnNumber" : 12,
    "endLineNumber" : 84,
    "endColumnNumber" : 31
  }, {
    "charLength" : 19,
    "startLineNumber" : 90,
    "startColumnNumber" : 28,
    "endLineNumber" : 90,
    "endColumnNumber" : 47
  }, {
    "charLength" : 19,
    "startLineNumber" : 108,
    "startColumnNumber" : 41,
    "endLineNumber" : 108,
    "endColumnNumber" : 60
  } ],
  "layoutRelationDataList" : [ {
    "firstKey" : 0,
    "secondKey" : 1,
    "layout" : 4
  }, {
    "firstKey" : 0,
    "secondKey" : 2,
    "layout" : 4
  }, {
    "firstKey" : 0,
    "secondKey" : 3,
    "layout" : 4
  }, {
    "firstKey" : 0,
    "secondKey" : 4,
    "layout" : 4
  }, {
    "firstKey" : 0,
    "secondKey" : 5,
    "layout" : 4
  }, {
    "firstKey" : 0,
    "secondKey" : 6,
    "layout" : 4
  }, {
    "firstKey" : 0,
    "secondKey" : 7,
    "layout" : 4
  }, {
    "firstKey" : 1,
    "secondKey" : 0,
    "layout" : 4
  }, {
    "firstKey" : 1,
    "secondKey" : 2,
    "layout" : 4
  }, {
    "firstKey" : 1,
    "secondKey" : 3,
    "layout" : 4
  }, {
    "firstKey" : 1,
    "secondKey" : 4,
    "layout" : 4
  }, {
    "firstKey" : 1,
    "secondKey" : 5,
    "layout" : 4
  }, {
    "firstKey" : 1,
    "secondKey" : 6,
    "layout" : 4
  }, {
    "firstKey" : 1,
    "secondKey" : 7,
    "layout" : 4
  }, {
    "firstKey" : 2,
    "secondKey" : 0,
    "layout" : 4
  }, {
    "firstKey" : 2,
    "secondKey" : 1,
    "layout" : 4
  }, {
    "firstKey" : 2,
    "secondKey" : 3,
    "layout" : 4
  }, {
    "firstKey" : 2,
    "secondKey" : 4,
    "layout" : 4
  }, {
    "firstKey" : 2,
    "secondKey" : 5,
    "layout" : 4
  }, {
    "firstKey" : 2,
    "secondKey" : 6,
    "layout" : 4
  }, {
    "firstKey" : 2,
    "secondKey" : 7,
    "layout" : 4
  }, {
    "firstKey" : 3,
    "secondKey" : 0,
    "layout" : 6
  }, {
    "firstKey" : 3,
    "secondKey" : 1,
    "layout" : 6
  }, {
    "firstKey" : 3,
    "secondKey" : 2,
    "layout" : 6
  }, {
    "firstKey" : 3,
    "secondKey" : 4,
    "layout" : 5
  }, {
    "firstKey" : 3,
    "secondKey" : 5,
    "layout" : 5
  }, {
    "firstKey" : 3,
    "secondKey" : 6,
    "layout" : 5
  }, {
    "firstKey" : 3,
    "secondKey" : 7,
    "layout" : 6
  }, {
    "firstKey" : 4,
    "secondKey" : 0,
    "layout" : 6
  }, {
    "firstKey" : 4,
    "secondKey" : 1,
    "layout" : 6
  }, {
    "firstKey" : 4,
    "secondKey" : 2,
    "layout" : 6
  }, {
    "firstKey" : 4,
    "secondKey" : 3,
    "layout" : 5
  }, {
    "firstKey" : 4,
    "secondKey" : 5,
    "layout" : 3
  }, {
    "firstKey" : 4,
    "secondKey" : 6,
    "layout" : 3
  }, {
    "firstKey" : 4,
    "secondKey" : 7,
    "layout" : 6
  }, {
    "firstKey" : 5,
    "secondKey" : 0,
    "layout" : 9
  }, {
    "firstKey" : 5,
    "secondKey" : 1,
    "layout" : 9
  }, {
    "firstKey" : 5,
    "secondKey" : 2,
    "layout" : 9
  }, {
    "firstKey" : 5,
    "secondKey" : 3,
    "layout" : 8
  }, {
    "firstKey" : 5,
    "secondKey" : 4,
    "layout" : 6
  }, {
    "firstKey" : 5,
    "secondKey" : 6,
    "layout" : 6
  }, {
    "firstKey" : 5,
    "secondKey" : 7,
    "layout" : 9
  }, {
    "firstKey" : 6,
    "secondKey" : 0,
    "layout" : 7
  }, {
    "firstKey" : 6,
    "secondKey" : 1,
    "layout" : 7
  }, {
    "firstKey" : 6,
    "secondKey" : 2,
    "layout" : 7
  }, {
    "firstKey" : 6,
    "secondKey" : 3,
    "layout" : 6
  }, {
    "firstKey" : 6,
    "secondKey" : 4,
    "layout" : 4
  }, {
    "firstKey" : 6,
    "secondKey" : 5,
    "layout" : 4
  }, {
    "firstKey" : 6,
    "secondKey" : 7,
    "layout" : 7
  }, {
    "firstKey" : 7,
    "secondKey" : 0,
    "layout" : 7
  }, {
    "firstKey" : 7,
    "secondKey" : 1,
    "layout" : 7
  }, {
    "firstKey" : 7,
    "secondKey" : 2,
    "layout" : 7
  }, {
    "firstKey" : 7,
    "secondKey" : 3,
    "layout" : 7
  }, {
    "firstKey" : 7,
    "secondKey" : 4,
    "layout" : 7
  }, {
    "firstKey" : 7,
    "secondKey" : 5,
    "layout" : 7
  }, {
    "firstKey" : 7,
    "secondKey" : 6,
    "layout" : 7
  } ]
}