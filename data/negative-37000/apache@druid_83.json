{
  "id" : 83,
  "expression" : "\"DateTime[%s], partition[%d], spec[%s]\"",
  "projectName" : "apache@druid",
  "commitID" : "f6662b4893a9b8cc8a0ebb77e63daddde882148d",
  "filePath" : "/indexing-hadoop/src/main/java/org/apache/druid/indexer/DetermineHashedPartitionsJob.java",
  "occurrences" : 1,
  "isArithmeticExpression" : 0,
  "isGetTypeMethod" : 0,
  "expressionList" : [ {
    "nodeContext" : "\"DateTime[%s], partition[%d], spec[%s]\"",
    "nodeType" : "StringLiteral",
    "nodePosition" : {
      "charLength" : 39,
      "startLineNumber" : 222,
      "startColumnNumber" : 21,
      "endLineNumber" : 222,
      "endColumnNumber" : 60
    },
    "astNodeNumber" : 1,
    "astHeight" : 1,
    "parentDataList" : [ {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
      "nodePosition" : {
        "charLength" : 80,
        "startLineNumber" : 222,
        "startColumnNumber" : 12,
        "endLineNumber" : 222,
        "endColumnNumber" : 92
      },
      "nodeContext" : "log.info(\"DateTime[%s], partition[%d], spec[%s]\",bucket,i,actualSpecs.get(i))",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 10,
      "astHeight" : 3
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 81,
        "startLineNumber" : 222,
        "startColumnNumber" : 12,
        "endLineNumber" : 222,
        "endColumnNumber" : 93
      },
      "nodeContext" : "log.info(\"DateTime[%s], partition[%d], spec[%s]\",bucket,i,actualSpecs.get(i));\n",
      "nodeType" : "ExpressionStatement",
      "astNodeNumber" : 11,
      "astHeight" : 4
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.ForStatement,body]",
      "nodePosition" : {
        "charLength" : 583,
        "startLineNumber" : 207,
        "startColumnNumber" : 51,
        "endLineNumber" : 223,
        "endColumnNumber" : 11
      },
      "nodeContext" : "{\n  actualSpecs.add(new HadoopyShardSpec(new HashBasedNumberedShardSpec(i,numberOfShards,i,numberOfShards,null,partitionFunction,HadoopDruidIndexerConfig.JSON_MAPPER),shardCount++));\n  log.info(\"DateTime[%s], partition[%d], spec[%s]\",bucket,i,actualSpecs.get(i));\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 33,
      "astHeight" : 7
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 624,
        "startLineNumber" : 207,
        "startColumnNumber" : 10,
        "endLineNumber" : 223,
        "endColumnNumber" : 11
      },
      "nodeContext" : "for (int i=0; i < numberOfShards; ++i) {\n  actualSpecs.add(new HadoopyShardSpec(new HashBasedNumberedShardSpec(i,numberOfShards,i,numberOfShards,null,partitionFunction,HadoopDruidIndexerConfig.JSON_MAPPER),shardCount++));\n  log.info(\"DateTime[%s], partition[%d], spec[%s]\",bucket,i,actualSpecs.get(i));\n}\n",
      "nodeType" : "ForStatement",
      "astNodeNumber" : 44,
      "astHeight" : 8
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
      "nodePosition" : {
        "charLength" : 1234,
        "startLineNumber" : 194,
        "startColumnNumber" : 69,
        "endLineNumber" : 227,
        "endColumnNumber" : 9
      },
      "nodeContext" : "{\n  final Long numRows=HadoopDruidIndexerConfig.JSON_MAPPER.readValue(Utils.openInputStream(groupByJob,partitionInfoPath),Long.class);\n  log.info(\"Found approximately [%,d] rows in data.\",numRows);\n  final int numberOfShards=(int)Math.ceil((double)numRows / config.getTargetPartitionSize());\n  log.info(\"Creating [%,d] shards\",numberOfShards);\n  List<HadoopyShardSpec> actualSpecs=Lists.newArrayListWithExpectedSize(numberOfShards);\n  for (int i=0; i < numberOfShards; ++i) {\n    actualSpecs.add(new HadoopyShardSpec(new HashBasedNumberedShardSpec(i,numberOfShards,i,numberOfShards,null,partitionFunction,HadoopDruidIndexerConfig.JSON_MAPPER),shardCount++));\n    log.info(\"DateTime[%s], partition[%d], spec[%s]\",bucket,i,actualSpecs.get(i));\n  }\n  shardSpecs.put(bucket.getMillis(),actualSpecs);\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 113,
      "astHeight" : 9
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 1378,
        "startLineNumber" : 194,
        "startColumnNumber" : 8,
        "endLineNumber" : 229,
        "endColumnNumber" : 9
      },
      "nodeContext" : "if (Utils.exists(groupByJob,fileSystem,partitionInfoPath)) {\n  final Long numRows=HadoopDruidIndexerConfig.JSON_MAPPER.readValue(Utils.openInputStream(groupByJob,partitionInfoPath),Long.class);\n  log.info(\"Found approximately [%,d] rows in data.\",numRows);\n  final int numberOfShards=(int)Math.ceil((double)numRows / config.getTargetPartitionSize());\n  log.info(\"Creating [%,d] shards\",numberOfShards);\n  List<HadoopyShardSpec> actualSpecs=Lists.newArrayListWithExpectedSize(numberOfShards);\n  for (int i=0; i < numberOfShards; ++i) {\n    actualSpecs.add(new HadoopyShardSpec(new HashBasedNumberedShardSpec(i,numberOfShards,i,numberOfShards,null,partitionFunction,HadoopDruidIndexerConfig.JSON_MAPPER),shardCount++));\n    log.info(\"DateTime[%s], partition[%d], spec[%s]\",bucket,i,actualSpecs.get(i));\n  }\n  shardSpecs.put(bucket.getMillis(),actualSpecs);\n}\n else {\n  log.info(\"Path[%s] didn't exist!?\",partitionInfoPath);\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 127,
      "astHeight" : 10
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.EnhancedForStatement,body]",
      "nodePosition" : {
        "charLength" : 1681,
        "startLineNumber" : 187,
        "startColumnNumber" : 79,
        "endLineNumber" : 230,
        "endColumnNumber" : 7
      },
      "nodeContext" : "{\n  DateTime bucket=segmentGranularity.getStart();\n  final Path partitionInfoPath=config.makeSegmentPartitionInfoPath(segmentGranularity);\n  if (fileSystem == null) {\n    fileSystem=partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n  }\n  if (Utils.exists(groupByJob,fileSystem,partitionInfoPath)) {\n    final Long numRows=HadoopDruidIndexerConfig.JSON_MAPPER.readValue(Utils.openInputStream(groupByJob,partitionInfoPath),Long.class);\n    log.info(\"Found approximately [%,d] rows in data.\",numRows);\n    final int numberOfShards=(int)Math.ceil((double)numRows / config.getTargetPartitionSize());\n    log.info(\"Creating [%,d] shards\",numberOfShards);\n    List<HadoopyShardSpec> actualSpecs=Lists.newArrayListWithExpectedSize(numberOfShards);\n    for (int i=0; i < numberOfShards; ++i) {\n      actualSpecs.add(new HadoopyShardSpec(new HashBasedNumberedShardSpec(i,numberOfShards,i,numberOfShards,null,partitionFunction,HadoopDruidIndexerConfig.JSON_MAPPER),shardCount++));\n      log.info(\"DateTime[%s], partition[%d], spec[%s]\",bucket,i,actualSpecs.get(i));\n    }\n    shardSpecs.put(bucket.getMillis(),actualSpecs);\n  }\n else {\n    log.info(\"Path[%s] didn't exist!?\",partitionInfoPath);\n  }\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 160,
      "astHeight" : 11
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 1754,
        "startLineNumber" : 187,
        "startColumnNumber" : 6,
        "endLineNumber" : 230,
        "endColumnNumber" : 7
      },
      "nodeContext" : "for (Interval segmentGranularity : config.getSegmentGranularIntervals()) {\n  DateTime bucket=segmentGranularity.getStart();\n  final Path partitionInfoPath=config.makeSegmentPartitionInfoPath(segmentGranularity);\n  if (fileSystem == null) {\n    fileSystem=partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n  }\n  if (Utils.exists(groupByJob,fileSystem,partitionInfoPath)) {\n    final Long numRows=HadoopDruidIndexerConfig.JSON_MAPPER.readValue(Utils.openInputStream(groupByJob,partitionInfoPath),Long.class);\n    log.info(\"Found approximately [%,d] rows in data.\",numRows);\n    final int numberOfShards=(int)Math.ceil((double)numRows / config.getTargetPartitionSize());\n    log.info(\"Creating [%,d] shards\",numberOfShards);\n    List<HadoopyShardSpec> actualSpecs=Lists.newArrayListWithExpectedSize(numberOfShards);\n    for (int i=0; i < numberOfShards; ++i) {\n      actualSpecs.add(new HadoopyShardSpec(new HashBasedNumberedShardSpec(i,numberOfShards,i,numberOfShards,null,partitionFunction,HadoopDruidIndexerConfig.JSON_MAPPER),shardCount++));\n      log.info(\"DateTime[%s], partition[%d], spec[%s]\",bucket,i,actualSpecs.get(i));\n    }\n    shardSpecs.put(bucket.getMillis(),actualSpecs);\n  }\n else {\n    log.info(\"Path[%s] didn't exist!?\",partitionInfoPath);\n  }\n}\n",
      "nodeType" : "EnhancedForStatement",
      "astNodeNumber" : 168,
      "astHeight" : 12
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.TryStatement,body]",
      "nodePosition" : {
        "charLength" : 6169,
        "startLineNumber" : 91,
        "startColumnNumber" : 8,
        "endLineNumber" : 239,
        "endColumnNumber" : 5
      },
      "nodeContext" : "{\n  startTime=System.currentTimeMillis();\n  groupByJob=Job.getInstance(new Configuration(),StringUtils.format(\"%s-determine_partitions_hashed-%s\",config.getDataSource(),config.getIntervals()));\n  JobHelper.injectSystemProperties(groupByJob.getConfiguration(),config);\n  config.addJobProperties(groupByJob);\n  groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n  groupByJob.setMapOutputKeyClass(LongWritable.class);\n  groupByJob.setMapOutputValueClass(BytesWritable.class);\n  groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n  groupByJob.setOutputKeyClass(NullWritable.class);\n  groupByJob.setOutputValueClass(NullWritable.class);\n  groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n  groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n  if (config.getInputIntervals().isEmpty()) {\n    groupByJob.setNumReduceTasks(1);\n  }\n else {\n    groupByJob.setNumReduceTasks(Iterators.size(config.getSegmentGranularIntervals().iterator()));\n  }\n  JobHelper.setupClasspath(JobHelper.distributedClassPath(config.getWorkingPath()),JobHelper.distributedClassPath(config.makeIntermediatePath()),groupByJob);\n  config.addInputPaths(groupByJob);\n  config.intoConfiguration(groupByJob);\n  FileOutputFormat.setOutputPath(groupByJob,config.makeGroupedDataDir());\n  groupByJob.submit();\n  log.info(\"Job %s submitted, status available at: %s\",groupByJob.getJobName(),groupByJob.getTrackingURL());\n  if (groupByJob.getJobID() != null) {\n    JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(),groupByJob.getJobID().toString());\n  }\n  try {\n    if (!groupByJob.waitForCompletion(true)) {\n      log.error(\"Job failed: %s\",groupByJob.getJobID());\n      failureCause=Utils.getFailureMessage(groupByJob,HadoopDruidIndexerConfig.JSON_MAPPER);\n      return false;\n    }\n  }\n catch (  IOException ioe) {\n    if (!Utils.checkAppSuccessForJobIOException(ioe,groupByJob,config.isUseYarnRMJobStatusFallback())) {\n      throw ioe;\n    }\n  }\n  log.info(\"Job completed, loading up partitions for intervals[%s].\",config.getSegmentGranularIntervals());\n  FileSystem fileSystem=null;\n  if (config.getInputIntervals().isEmpty()) {\n    final Path intervalInfoPath=config.makeIntervalInfoPath();\n    fileSystem=intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n    if (!Utils.exists(groupByJob,fileSystem,intervalInfoPath)) {\n      throw new ISE(\"Path[%s] didn't exist!?\",intervalInfoPath);\n    }\n    List<Interval> intervals=HadoopDruidIndexerConfig.JSON_MAPPER.readValue(Utils.openInputStream(groupByJob,intervalInfoPath),new TypeReference<List<Interval>>(){\n    }\n);\n    config.setGranularitySpec(new UniformGranularitySpec(config.getGranularitySpec().getSegmentGranularity(),config.getGranularitySpec().getQueryGranularity(),config.getGranularitySpec().isRollup(),intervals));\n    log.info(\"Determined Intervals for Job [%s].\",config.getSegmentGranularIntervals());\n  }\n  Map<Long,List<HadoopyShardSpec>> shardSpecs=new TreeMap<>(DateTimeComparator.getInstance());\n  PartitionsSpec partitionsSpec=config.getPartitionsSpec();\n  if (!(partitionsSpec instanceof HashedPartitionsSpec)) {\n    throw new ISE(\"%s is expected, but got %s\",HashedPartitionsSpec.class.getName(),partitionsSpec.getClass().getName());\n  }\n  HashPartitionFunction partitionFunction=((HashedPartitionsSpec)partitionsSpec).getPartitionFunction();\n  int shardCount=0;\n  for (  Interval segmentGranularity : config.getSegmentGranularIntervals()) {\n    DateTime bucket=segmentGranularity.getStart();\n    final Path partitionInfoPath=config.makeSegmentPartitionInfoPath(segmentGranularity);\n    if (fileSystem == null) {\n      fileSystem=partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n    }\n    if (Utils.exists(groupByJob,fileSystem,partitionInfoPath)) {\n      final Long numRows=HadoopDruidIndexerConfig.JSON_MAPPER.readValue(Utils.openInputStream(groupByJob,partitionInfoPath),Long.class);\n      log.info(\"Found approximately [%,d] rows in data.\",numRows);\n      final int numberOfShards=(int)Math.ceil((double)numRows / config.getTargetPartitionSize());\n      log.info(\"Creating [%,d] shards\",numberOfShards);\n      List<HadoopyShardSpec> actualSpecs=Lists.newArrayListWithExpectedSize(numberOfShards);\n      for (int i=0; i < numberOfShards; ++i) {\n        actualSpecs.add(new HadoopyShardSpec(new HashBasedNumberedShardSpec(i,numberOfShards,i,numberOfShards,null,partitionFunction,HadoopDruidIndexerConfig.JSON_MAPPER),shardCount++));\n        log.info(\"DateTime[%s], partition[%d], spec[%s]\",bucket,i,actualSpecs.get(i));\n      }\n      shardSpecs.put(bucket.getMillis(),actualSpecs);\n    }\n else {\n      log.info(\"Path[%s] didn't exist!?\",partitionInfoPath);\n    }\n  }\n  config.setShardSpecs(shardSpecs);\n  log.info(\"DetermineHashedPartitionsJob took %d millis\",(System.currentTimeMillis() - startTime));\n  return true;\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 604,
      "astHeight" : 13
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 6242,
        "startLineNumber" : 91,
        "startColumnNumber" : 4,
        "endLineNumber" : 242,
        "endColumnNumber" : 5
      },
      "nodeContext" : "try {\n  startTime=System.currentTimeMillis();\n  groupByJob=Job.getInstance(new Configuration(),StringUtils.format(\"%s-determine_partitions_hashed-%s\",config.getDataSource(),config.getIntervals()));\n  JobHelper.injectSystemProperties(groupByJob.getConfiguration(),config);\n  config.addJobProperties(groupByJob);\n  groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n  groupByJob.setMapOutputKeyClass(LongWritable.class);\n  groupByJob.setMapOutputValueClass(BytesWritable.class);\n  groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n  groupByJob.setOutputKeyClass(NullWritable.class);\n  groupByJob.setOutputValueClass(NullWritable.class);\n  groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n  groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n  if (config.getInputIntervals().isEmpty()) {\n    groupByJob.setNumReduceTasks(1);\n  }\n else {\n    groupByJob.setNumReduceTasks(Iterators.size(config.getSegmentGranularIntervals().iterator()));\n  }\n  JobHelper.setupClasspath(JobHelper.distributedClassPath(config.getWorkingPath()),JobHelper.distributedClassPath(config.makeIntermediatePath()),groupByJob);\n  config.addInputPaths(groupByJob);\n  config.intoConfiguration(groupByJob);\n  FileOutputFormat.setOutputPath(groupByJob,config.makeGroupedDataDir());\n  groupByJob.submit();\n  log.info(\"Job %s submitted, status available at: %s\",groupByJob.getJobName(),groupByJob.getTrackingURL());\n  if (groupByJob.getJobID() != null) {\n    JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(),groupByJob.getJobID().toString());\n  }\n  try {\n    if (!groupByJob.waitForCompletion(true)) {\n      log.error(\"Job failed: %s\",groupByJob.getJobID());\n      failureCause=Utils.getFailureMessage(groupByJob,HadoopDruidIndexerConfig.JSON_MAPPER);\n      return false;\n    }\n  }\n catch (  IOException ioe) {\n    if (!Utils.checkAppSuccessForJobIOException(ioe,groupByJob,config.isUseYarnRMJobStatusFallback())) {\n      throw ioe;\n    }\n  }\n  log.info(\"Job completed, loading up partitions for intervals[%s].\",config.getSegmentGranularIntervals());\n  FileSystem fileSystem=null;\n  if (config.getInputIntervals().isEmpty()) {\n    final Path intervalInfoPath=config.makeIntervalInfoPath();\n    fileSystem=intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n    if (!Utils.exists(groupByJob,fileSystem,intervalInfoPath)) {\n      throw new ISE(\"Path[%s] didn't exist!?\",intervalInfoPath);\n    }\n    List<Interval> intervals=HadoopDruidIndexerConfig.JSON_MAPPER.readValue(Utils.openInputStream(groupByJob,intervalInfoPath),new TypeReference<List<Interval>>(){\n    }\n);\n    config.setGranularitySpec(new UniformGranularitySpec(config.getGranularitySpec().getSegmentGranularity(),config.getGranularitySpec().getQueryGranularity(),config.getGranularitySpec().isRollup(),intervals));\n    log.info(\"Determined Intervals for Job [%s].\",config.getSegmentGranularIntervals());\n  }\n  Map<Long,List<HadoopyShardSpec>> shardSpecs=new TreeMap<>(DateTimeComparator.getInstance());\n  PartitionsSpec partitionsSpec=config.getPartitionsSpec();\n  if (!(partitionsSpec instanceof HashedPartitionsSpec)) {\n    throw new ISE(\"%s is expected, but got %s\",HashedPartitionsSpec.class.getName(),partitionsSpec.getClass().getName());\n  }\n  HashPartitionFunction partitionFunction=((HashedPartitionsSpec)partitionsSpec).getPartitionFunction();\n  int shardCount=0;\n  for (  Interval segmentGranularity : config.getSegmentGranularIntervals()) {\n    DateTime bucket=segmentGranularity.getStart();\n    final Path partitionInfoPath=config.makeSegmentPartitionInfoPath(segmentGranularity);\n    if (fileSystem == null) {\n      fileSystem=partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n    }\n    if (Utils.exists(groupByJob,fileSystem,partitionInfoPath)) {\n      final Long numRows=HadoopDruidIndexerConfig.JSON_MAPPER.readValue(Utils.openInputStream(groupByJob,partitionInfoPath),Long.class);\n      log.info(\"Found approximately [%,d] rows in data.\",numRows);\n      final int numberOfShards=(int)Math.ceil((double)numRows / config.getTargetPartitionSize());\n      log.info(\"Creating [%,d] shards\",numberOfShards);\n      List<HadoopyShardSpec> actualSpecs=Lists.newArrayListWithExpectedSize(numberOfShards);\n      for (int i=0; i < numberOfShards; ++i) {\n        actualSpecs.add(new HadoopyShardSpec(new HashBasedNumberedShardSpec(i,numberOfShards,i,numberOfShards,null,partitionFunction,HadoopDruidIndexerConfig.JSON_MAPPER),shardCount++));\n        log.info(\"DateTime[%s], partition[%d], spec[%s]\",bucket,i,actualSpecs.get(i));\n      }\n      shardSpecs.put(bucket.getMillis(),actualSpecs);\n    }\n else {\n      log.info(\"Path[%s] didn't exist!?\",partitionInfoPath);\n    }\n  }\n  config.setShardSpecs(shardSpecs);\n  log.info(\"DetermineHashedPartitionsJob took %d millis\",(System.currentTimeMillis() - startTime));\n  return true;\n}\n catch (Exception e) {\n  throw new RuntimeException(e);\n}\n",
      "nodeType" : "TryStatement",
      "astNodeNumber" : 616,
      "astHeight" : 14
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodDeclaration,body]",
      "nodePosition" : {
        "charLength" : 6252,
        "startLineNumber" : 90,
        "startColumnNumber" : 2,
        "endLineNumber" : 243,
        "endColumnNumber" : 3
      },
      "nodeContext" : "{\n  try {\n    startTime=System.currentTimeMillis();\n    groupByJob=Job.getInstance(new Configuration(),StringUtils.format(\"%s-determine_partitions_hashed-%s\",config.getDataSource(),config.getIntervals()));\n    JobHelper.injectSystemProperties(groupByJob.getConfiguration(),config);\n    config.addJobProperties(groupByJob);\n    groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n    groupByJob.setMapOutputKeyClass(LongWritable.class);\n    groupByJob.setMapOutputValueClass(BytesWritable.class);\n    groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n    groupByJob.setOutputKeyClass(NullWritable.class);\n    groupByJob.setOutputValueClass(NullWritable.class);\n    groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n    groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n    if (config.getInputIntervals().isEmpty()) {\n      groupByJob.setNumReduceTasks(1);\n    }\n else {\n      groupByJob.setNumReduceTasks(Iterators.size(config.getSegmentGranularIntervals().iterator()));\n    }\n    JobHelper.setupClasspath(JobHelper.distributedClassPath(config.getWorkingPath()),JobHelper.distributedClassPath(config.makeIntermediatePath()),groupByJob);\n    config.addInputPaths(groupByJob);\n    config.intoConfiguration(groupByJob);\n    FileOutputFormat.setOutputPath(groupByJob,config.makeGroupedDataDir());\n    groupByJob.submit();\n    log.info(\"Job %s submitted, status available at: %s\",groupByJob.getJobName(),groupByJob.getTrackingURL());\n    if (groupByJob.getJobID() != null) {\n      JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(),groupByJob.getJobID().toString());\n    }\n    try {\n      if (!groupByJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\",groupByJob.getJobID());\n        failureCause=Utils.getFailureMessage(groupByJob,HadoopDruidIndexerConfig.JSON_MAPPER);\n        return false;\n      }\n    }\n catch (    IOException ioe) {\n      if (!Utils.checkAppSuccessForJobIOException(ioe,groupByJob,config.isUseYarnRMJobStatusFallback())) {\n        throw ioe;\n      }\n    }\n    log.info(\"Job completed, loading up partitions for intervals[%s].\",config.getSegmentGranularIntervals());\n    FileSystem fileSystem=null;\n    if (config.getInputIntervals().isEmpty()) {\n      final Path intervalInfoPath=config.makeIntervalInfoPath();\n      fileSystem=intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n      if (!Utils.exists(groupByJob,fileSystem,intervalInfoPath)) {\n        throw new ISE(\"Path[%s] didn't exist!?\",intervalInfoPath);\n      }\n      List<Interval> intervals=HadoopDruidIndexerConfig.JSON_MAPPER.readValue(Utils.openInputStream(groupByJob,intervalInfoPath),new TypeReference<List<Interval>>(){\n      }\n);\n      config.setGranularitySpec(new UniformGranularitySpec(config.getGranularitySpec().getSegmentGranularity(),config.getGranularitySpec().getQueryGranularity(),config.getGranularitySpec().isRollup(),intervals));\n      log.info(\"Determined Intervals for Job [%s].\",config.getSegmentGranularIntervals());\n    }\n    Map<Long,List<HadoopyShardSpec>> shardSpecs=new TreeMap<>(DateTimeComparator.getInstance());\n    PartitionsSpec partitionsSpec=config.getPartitionsSpec();\n    if (!(partitionsSpec instanceof HashedPartitionsSpec)) {\n      throw new ISE(\"%s is expected, but got %s\",HashedPartitionsSpec.class.getName(),partitionsSpec.getClass().getName());\n    }\n    HashPartitionFunction partitionFunction=((HashedPartitionsSpec)partitionsSpec).getPartitionFunction();\n    int shardCount=0;\n    for (    Interval segmentGranularity : config.getSegmentGranularIntervals()) {\n      DateTime bucket=segmentGranularity.getStart();\n      final Path partitionInfoPath=config.makeSegmentPartitionInfoPath(segmentGranularity);\n      if (fileSystem == null) {\n        fileSystem=partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n      }\n      if (Utils.exists(groupByJob,fileSystem,partitionInfoPath)) {\n        final Long numRows=HadoopDruidIndexerConfig.JSON_MAPPER.readValue(Utils.openInputStream(groupByJob,partitionInfoPath),Long.class);\n        log.info(\"Found approximately [%,d] rows in data.\",numRows);\n        final int numberOfShards=(int)Math.ceil((double)numRows / config.getTargetPartitionSize());\n        log.info(\"Creating [%,d] shards\",numberOfShards);\n        List<HadoopyShardSpec> actualSpecs=Lists.newArrayListWithExpectedSize(numberOfShards);\n        for (int i=0; i < numberOfShards; ++i) {\n          actualSpecs.add(new HadoopyShardSpec(new HashBasedNumberedShardSpec(i,numberOfShards,i,numberOfShards,null,partitionFunction,HadoopDruidIndexerConfig.JSON_MAPPER),shardCount++));\n          log.info(\"DateTime[%s], partition[%d], spec[%s]\",bucket,i,actualSpecs.get(i));\n        }\n        shardSpecs.put(bucket.getMillis(),actualSpecs);\n      }\n else {\n        log.info(\"Path[%s] didn't exist!?\",partitionInfoPath);\n      }\n    }\n    config.setShardSpecs(shardSpecs);\n    log.info(\"DetermineHashedPartitionsJob took %d millis\",(System.currentTimeMillis() - startTime));\n    return true;\n  }\n catch (  Exception e) {\n    throw new RuntimeException(e);\n  }\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 617,
      "astHeight" : 15
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.TypeDeclaration,bodyDeclarations]",
      "nodePosition" : {
        "charLength" : 6287,
        "startLineNumber" : 88,
        "startColumnNumber" : 2,
        "endLineNumber" : 243,
        "endColumnNumber" : 3
      },
      "nodeContext" : "@Override public boolean run(){\n  try {\n    startTime=System.currentTimeMillis();\n    groupByJob=Job.getInstance(new Configuration(),StringUtils.format(\"%s-determine_partitions_hashed-%s\",config.getDataSource(),config.getIntervals()));\n    JobHelper.injectSystemProperties(groupByJob.getConfiguration(),config);\n    config.addJobProperties(groupByJob);\n    groupByJob.setMapperClass(DetermineCardinalityMapper.class);\n    groupByJob.setMapOutputKeyClass(LongWritable.class);\n    groupByJob.setMapOutputValueClass(BytesWritable.class);\n    groupByJob.setReducerClass(DetermineCardinalityReducer.class);\n    groupByJob.setOutputKeyClass(NullWritable.class);\n    groupByJob.setOutputValueClass(NullWritable.class);\n    groupByJob.setOutputFormatClass(SequenceFileOutputFormat.class);\n    groupByJob.setPartitionerClass(DetermineHashedPartitionsPartitioner.class);\n    if (config.getInputIntervals().isEmpty()) {\n      groupByJob.setNumReduceTasks(1);\n    }\n else {\n      groupByJob.setNumReduceTasks(Iterators.size(config.getSegmentGranularIntervals().iterator()));\n    }\n    JobHelper.setupClasspath(JobHelper.distributedClassPath(config.getWorkingPath()),JobHelper.distributedClassPath(config.makeIntermediatePath()),groupByJob);\n    config.addInputPaths(groupByJob);\n    config.intoConfiguration(groupByJob);\n    FileOutputFormat.setOutputPath(groupByJob,config.makeGroupedDataDir());\n    groupByJob.submit();\n    log.info(\"Job %s submitted, status available at: %s\",groupByJob.getJobName(),groupByJob.getTrackingURL());\n    if (groupByJob.getJobID() != null) {\n      JobHelper.writeJobIdToFile(config.getHadoopJobIdFileName(),groupByJob.getJobID().toString());\n    }\n    try {\n      if (!groupByJob.waitForCompletion(true)) {\n        log.error(\"Job failed: %s\",groupByJob.getJobID());\n        failureCause=Utils.getFailureMessage(groupByJob,HadoopDruidIndexerConfig.JSON_MAPPER);\n        return false;\n      }\n    }\n catch (    IOException ioe) {\n      if (!Utils.checkAppSuccessForJobIOException(ioe,groupByJob,config.isUseYarnRMJobStatusFallback())) {\n        throw ioe;\n      }\n    }\n    log.info(\"Job completed, loading up partitions for intervals[%s].\",config.getSegmentGranularIntervals());\n    FileSystem fileSystem=null;\n    if (config.getInputIntervals().isEmpty()) {\n      final Path intervalInfoPath=config.makeIntervalInfoPath();\n      fileSystem=intervalInfoPath.getFileSystem(groupByJob.getConfiguration());\n      if (!Utils.exists(groupByJob,fileSystem,intervalInfoPath)) {\n        throw new ISE(\"Path[%s] didn't exist!?\",intervalInfoPath);\n      }\n      List<Interval> intervals=HadoopDruidIndexerConfig.JSON_MAPPER.readValue(Utils.openInputStream(groupByJob,intervalInfoPath),new TypeReference<List<Interval>>(){\n      }\n);\n      config.setGranularitySpec(new UniformGranularitySpec(config.getGranularitySpec().getSegmentGranularity(),config.getGranularitySpec().getQueryGranularity(),config.getGranularitySpec().isRollup(),intervals));\n      log.info(\"Determined Intervals for Job [%s].\",config.getSegmentGranularIntervals());\n    }\n    Map<Long,List<HadoopyShardSpec>> shardSpecs=new TreeMap<>(DateTimeComparator.getInstance());\n    PartitionsSpec partitionsSpec=config.getPartitionsSpec();\n    if (!(partitionsSpec instanceof HashedPartitionsSpec)) {\n      throw new ISE(\"%s is expected, but got %s\",HashedPartitionsSpec.class.getName(),partitionsSpec.getClass().getName());\n    }\n    HashPartitionFunction partitionFunction=((HashedPartitionsSpec)partitionsSpec).getPartitionFunction();\n    int shardCount=0;\n    for (    Interval segmentGranularity : config.getSegmentGranularIntervals()) {\n      DateTime bucket=segmentGranularity.getStart();\n      final Path partitionInfoPath=config.makeSegmentPartitionInfoPath(segmentGranularity);\n      if (fileSystem == null) {\n        fileSystem=partitionInfoPath.getFileSystem(groupByJob.getConfiguration());\n      }\n      if (Utils.exists(groupByJob,fileSystem,partitionInfoPath)) {\n        final Long numRows=HadoopDruidIndexerConfig.JSON_MAPPER.readValue(Utils.openInputStream(groupByJob,partitionInfoPath),Long.class);\n        log.info(\"Found approximately [%,d] rows in data.\",numRows);\n        final int numberOfShards=(int)Math.ceil((double)numRows / config.getTargetPartitionSize());\n        log.info(\"Creating [%,d] shards\",numberOfShards);\n        List<HadoopyShardSpec> actualSpecs=Lists.newArrayListWithExpectedSize(numberOfShards);\n        for (int i=0; i < numberOfShards; ++i) {\n          actualSpecs.add(new HadoopyShardSpec(new HashBasedNumberedShardSpec(i,numberOfShards,i,numberOfShards,null,partitionFunction,HadoopDruidIndexerConfig.JSON_MAPPER),shardCount++));\n          log.info(\"DateTime[%s], partition[%d], spec[%s]\",bucket,i,actualSpecs.get(i));\n        }\n        shardSpecs.put(bucket.getMillis(),actualSpecs);\n      }\n else {\n        log.info(\"Path[%s] didn't exist!?\",partitionInfoPath);\n      }\n    }\n    config.setShardSpecs(shardSpecs);\n    log.info(\"DetermineHashedPartitionsJob took %d millis\",(System.currentTimeMillis() - startTime));\n    return true;\n  }\n catch (  Exception e) {\n    throw new RuntimeException(e);\n  }\n}\n",
      "nodeType" : "MethodDeclaration",
      "astNodeNumber" : 623,
      "astHeight" : 16
    } ],
    "currentLineData" : {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 81,
        "startLineNumber" : 222,
        "startColumnNumber" : 12,
        "endLineNumber" : 222,
        "endColumnNumber" : 93
      },
      "nodeContext" : "log.info(\"DateTime[%s], partition[%d], spec[%s]\",bucket,i,actualSpecs.get(i));\n",
      "nodeType" : "ExpressionStatement",
      "astNodeNumber" : 11,
      "astHeight" : 4
    },
    "tokenLength" : 6,
    "type" : "java.lang.String"
  } ],
  "positionList" : [ {
    "charLength" : 39,
    "startLineNumber" : 222,
    "startColumnNumber" : 21,
    "endLineNumber" : 222,
    "endColumnNumber" : 60
  } ],
  "layoutRelationDataList" : [ ]
}