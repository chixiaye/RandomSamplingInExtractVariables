{
  "id" : 10,
  "expression" : "segmentsPerInterval.size()",
  "projectName" : "apache@druid",
  "commitID" : "2df42143aec6c50e9ac31d89cd75749d10d37a3d",
  "filePath" : "server/src/main/java/org/apache/druid/segment/realtime/appenderator/SegmentPublisherHelper.java",
  "occurrences" : 4,
  "expressionList" : [ {
    "nodeContext" : "segmentsPerInterval.size()",
    "nodeType" : "MethodInvocation",
    "nodePosition" : {
      "charLength" : 26,
      "startLineNumber" : 86,
      "startColumnNumber" : 78,
      "endLineNumber" : 86,
      "endColumnNumber" : 104
    },
    "astNodeNumber" : 3,
    "astHeight" : 2,
    "parentDataList" : [ {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.LambdaExpression,body]",
      "nodePosition" : {
        "charLength" : 69,
        "startLineNumber" : 86,
        "startColumnNumber" : 35,
        "endLineNumber" : 86,
        "endColumnNumber" : 104
      },
      "nodeContext" : "segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()",
      "nodeType" : "InfixExpression",
      "astNodeNumber" : 9,
      "astHeight" : 4
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.MethodInvocation,arguments]",
      "nodePosition" : {
        "charLength" : 80,
        "startLineNumber" : 86,
        "startColumnNumber" : 24,
        "endLineNumber" : 86,
        "endColumnNumber" : 104
      },
      "nodeContext" : "segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()",
      "nodeType" : "LambdaExpression",
      "astNodeNumber" : 12,
      "astHeight" : 5
    } ],
    "tokenLength" : 2,
    "type" : "int"
  }, {
    "nodeContext" : "segmentsPerInterval.size()",
    "nodeType" : "MethodInvocation",
    "nodePosition" : {
      "charLength" : 26,
      "startLineNumber" : 89,
      "startColumnNumber" : 12,
      "endLineNumber" : 89,
      "endColumnNumber" : 38
    },
    "astNodeNumber" : 3,
    "astHeight" : 2,
    "parentDataList" : [ {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,expression]",
      "nodePosition" : {
        "charLength" : 56,
        "startLineNumber" : 89,
        "startColumnNumber" : 12,
        "endLineNumber" : 89,
        "endColumnNumber" : 68
      },
      "nodeContext" : "segmentsPerInterval.size() != actualCorePartitionSetSize",
      "nodeType" : "InfixExpression",
      "astNodeNumber" : 5,
      "astHeight" : 3
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 459,
        "startLineNumber" : 89,
        "startColumnNumber" : 8,
        "endLineNumber" : 99,
        "endColumnNumber" : 9
      },
      "nodeContext" : "if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n  LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n  throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 24,
      "astHeight" : 6
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
      "nodePosition" : {
        "charLength" : 782,
        "startLineNumber" : 77,
        "startColumnNumber" : 62,
        "endLineNumber" : 101,
        "endColumnNumber" : 7
      },
      "nodeContext" : "{\n  int segmentsPerInterval.size()=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n  if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 62,
      "astHeight" : 11
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,elseStatement]",
      "nodePosition" : {
        "charLength" : 1003,
        "startLineNumber" : 77,
        "startColumnNumber" : 13,
        "endLineNumber" : 105,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (firstShardSpec instanceof BuildingShardSpec) {\n  int segmentsPerInterval.size()=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n  if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n}\n else if (firstShardSpec instanceof BucketNumberedShardSpec) {\n  throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n}\n else {\n  annotateFn=null;\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 84,
      "astHeight" : 12
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 1133,
        "startLineNumber" : 75,
        "startColumnNumber" : 6,
        "endLineNumber" : 105,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (firstShardSpec instanceof OverwriteShardSpec) {\n  annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n}\n else if (firstShardSpec instanceof BuildingShardSpec) {\n  int segmentsPerInterval.size()=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n  if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n}\n else if (firstShardSpec instanceof BucketNumberedShardSpec) {\n  throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n}\n else {\n  annotateFn=null;\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 98,
      "astHeight" : 13
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.EnhancedForStatement,body]",
      "nodePosition" : {
        "charLength" : 1824,
        "startLineNumber" : 60,
        "startColumnNumber" : 83,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "{\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int segmentsPerInterval.size()=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n    if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 196,
      "astHeight" : 14
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 1902,
        "startLineNumber" : 60,
        "startColumnNumber" : 4,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "for (Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int segmentsPerInterval.size()=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n    if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "EnhancedForStatement",
      "astNodeNumber" : 212,
      "astHeight" : 15
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodDeclaration,body]",
      "nodePosition" : {
        "charLength" : 2273,
        "startLineNumber" : 54,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "{\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int segmentsPerInterval.size()=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n      if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 270,
      "astHeight" : 16
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.TypeDeclaration,bodyDeclarations]",
      "nodePosition" : {
        "charLength" : 2785,
        "startLineNumber" : 45,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "/** \n * This method fills missing information in the shard spec if necessary when publishing segments. - When time chunk lock is used, the non-appending task should set the proper size of the core partitions for dynamically-partitioned segments. See  {@link #annotateCorePartitionSetSizeFn}. - When segment lock is used, the overwriting task should set the proper size of the atomic update group. See  {@link #annotateAtomicUpdateGroupFn}.\n */\nstatic Set<DataSegment> annotateShardSpec(Set<DataSegment> segments){\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int segmentsPerInterval.size()=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n      if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "MethodDeclaration",
      "astNodeNumber" : 286,
      "astHeight" : 17
    } ],
    "tokenLength" : 2,
    "type" : "int"
  }, {
    "nodeContext" : "segmentsPerInterval.size()",
    "nodeType" : "MethodInvocation",
    "nodePosition" : {
      "charLength" : 26,
      "startLineNumber" : 96,
      "startColumnNumber" : 14,
      "endLineNumber" : 96,
      "endColumnNumber" : 40
    },
    "astNodeNumber" : 3,
    "astHeight" : 2,
    "parentDataList" : [ {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.ThrowStatement,expression]",
      "nodePosition" : {
        "charLength" : 286,
        "startLineNumber" : 91,
        "startColumnNumber" : 16,
        "endLineNumber" : 98,
        "endColumnNumber" : 11
      },
      "nodeContext" : "new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize)",
      "nodeType" : "ClassInstanceCreation",
      "astNodeNumber" : 12,
      "astHeight" : 3
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 294,
        "startLineNumber" : 91,
        "startColumnNumber" : 10,
        "endLineNumber" : 98,
        "endColumnNumber" : 12
      },
      "nodeContext" : "throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n",
      "nodeType" : "ThrowStatement",
      "astNodeNumber" : 13,
      "astHeight" : 4
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
      "nodePosition" : {
        "charLength" : 397,
        "startLineNumber" : 89,
        "startColumnNumber" : 72,
        "endLineNumber" : 99,
        "endColumnNumber" : 9
      },
      "nodeContext" : "{\n  LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n  throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 20,
      "astHeight" : 5
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 459,
        "startLineNumber" : 89,
        "startColumnNumber" : 8,
        "endLineNumber" : 99,
        "endColumnNumber" : 9
      },
      "nodeContext" : "if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n  LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n  throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 24,
      "astHeight" : 6
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
      "nodePosition" : {
        "charLength" : 782,
        "startLineNumber" : 77,
        "startColumnNumber" : 62,
        "endLineNumber" : 101,
        "endColumnNumber" : 7
      },
      "nodeContext" : "{\n  int segmentsPerInterval.size()=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n  if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 62,
      "astHeight" : 11
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,elseStatement]",
      "nodePosition" : {
        "charLength" : 1003,
        "startLineNumber" : 77,
        "startColumnNumber" : 13,
        "endLineNumber" : 105,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (firstShardSpec instanceof BuildingShardSpec) {\n  int segmentsPerInterval.size()=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n  if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n}\n else if (firstShardSpec instanceof BucketNumberedShardSpec) {\n  throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n}\n else {\n  annotateFn=null;\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 84,
      "astHeight" : 12
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 1133,
        "startLineNumber" : 75,
        "startColumnNumber" : 6,
        "endLineNumber" : 105,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (firstShardSpec instanceof OverwriteShardSpec) {\n  annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n}\n else if (firstShardSpec instanceof BuildingShardSpec) {\n  int segmentsPerInterval.size()=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n  if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n}\n else if (firstShardSpec instanceof BucketNumberedShardSpec) {\n  throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n}\n else {\n  annotateFn=null;\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 98,
      "astHeight" : 13
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.EnhancedForStatement,body]",
      "nodePosition" : {
        "charLength" : 1824,
        "startLineNumber" : 60,
        "startColumnNumber" : 83,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "{\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int segmentsPerInterval.size()=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n    if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 196,
      "astHeight" : 14
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 1902,
        "startLineNumber" : 60,
        "startColumnNumber" : 4,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "for (Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int segmentsPerInterval.size()=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n    if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "EnhancedForStatement",
      "astNodeNumber" : 212,
      "astHeight" : 15
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodDeclaration,body]",
      "nodePosition" : {
        "charLength" : 2273,
        "startLineNumber" : 54,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "{\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int segmentsPerInterval.size()=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n      if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 270,
      "astHeight" : 16
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.TypeDeclaration,bodyDeclarations]",
      "nodePosition" : {
        "charLength" : 2785,
        "startLineNumber" : 45,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "/** \n * This method fills missing information in the shard spec if necessary when publishing segments. - When time chunk lock is used, the non-appending task should set the proper size of the core partitions for dynamically-partitioned segments. See  {@link #annotateCorePartitionSetSizeFn}. - When segment lock is used, the overwriting task should set the proper size of the atomic update group. See  {@link #annotateAtomicUpdateGroupFn}.\n */\nstatic Set<DataSegment> annotateShardSpec(Set<DataSegment> segments){\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int segmentsPerInterval.size()=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n      if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "MethodDeclaration",
      "astNodeNumber" : 286,
      "astHeight" : 17
    } ],
    "tokenLength" : 2,
    "type" : "int"
  }, {
    "nodeContext" : "segmentsPerInterval.size()",
    "nodeType" : "MethodInvocation",
    "nodePosition" : {
      "charLength" : 26,
      "startLineNumber" : 100,
      "startColumnNumber" : 52,
      "endLineNumber" : 100,
      "endColumnNumber" : 78
    },
    "astNodeNumber" : 3,
    "astHeight" : 2,
    "parentDataList" : [ {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.Assignment,rightHandSide]",
      "nodePosition" : {
        "charLength" : 58,
        "startLineNumber" : 100,
        "startColumnNumber" : 21,
        "endLineNumber" : 100,
        "endColumnNumber" : 79
      },
      "nodeContext" : "annotateCorePartitionSetSizeFn(segmentsPerInterval.size())",
      "nodeType" : "MethodInvocation",
      "astNodeNumber" : 5,
      "astHeight" : 3
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.ExpressionStatement,expression]",
      "nodePosition" : {
        "charLength" : 69,
        "startLineNumber" : 100,
        "startColumnNumber" : 8,
        "endLineNumber" : 100,
        "endColumnNumber" : 77
      },
      "nodeContext" : "annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size())",
      "nodeType" : "Assignment",
      "astNodeNumber" : 7,
      "astHeight" : 4
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 71,
        "startLineNumber" : 100,
        "startColumnNumber" : 8,
        "endLineNumber" : 100,
        "endColumnNumber" : 79
      },
      "nodeContext" : "annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n",
      "nodeType" : "ExpressionStatement",
      "astNodeNumber" : 8,
      "astHeight" : 5
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,thenStatement]",
      "nodePosition" : {
        "charLength" : 782,
        "startLineNumber" : 77,
        "startColumnNumber" : 62,
        "endLineNumber" : 101,
        "endColumnNumber" : 7
      },
      "nodeContext" : "{\n  int segmentsPerInterval.size()=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n  if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 62,
      "astHeight" : 11
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.IfStatement,elseStatement]",
      "nodePosition" : {
        "charLength" : 1003,
        "startLineNumber" : 77,
        "startColumnNumber" : 13,
        "endLineNumber" : 105,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (firstShardSpec instanceof BuildingShardSpec) {\n  int segmentsPerInterval.size()=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n  if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n}\n else if (firstShardSpec instanceof BucketNumberedShardSpec) {\n  throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n}\n else {\n  annotateFn=null;\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 84,
      "astHeight" : 12
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 1133,
        "startLineNumber" : 75,
        "startColumnNumber" : 6,
        "endLineNumber" : 105,
        "endColumnNumber" : 7
      },
      "nodeContext" : "if (firstShardSpec instanceof OverwriteShardSpec) {\n  annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n}\n else if (firstShardSpec instanceof BuildingShardSpec) {\n  int segmentsPerInterval.size()=segmentsPerInterval.size();\n  int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n  if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n    LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n    throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n  }\n  annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n}\n else if (firstShardSpec instanceof BucketNumberedShardSpec) {\n  throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n}\n else {\n  annotateFn=null;\n}\n",
      "nodeType" : "IfStatement",
      "astNodeNumber" : 98,
      "astHeight" : 13
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.EnhancedForStatement,body]",
      "nodePosition" : {
        "charLength" : 1824,
        "startLineNumber" : 60,
        "startColumnNumber" : 83,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "{\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int segmentsPerInterval.size()=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n    if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 196,
      "astHeight" : 14
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.Block,statements]",
      "nodePosition" : {
        "charLength" : 1902,
        "startLineNumber" : 60,
        "startColumnNumber" : 4,
        "endLineNumber" : 110,
        "endColumnNumber" : 5
      },
      "nodeContext" : "for (Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n  final Interval interval=entry.getKey();\n  final List<DataSegment> segmentsPerInterval=entry.getValue();\n  final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n  final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n  if (anyMismatch) {\n    throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n  }\n  final Function<DataSegment,DataSegment> annotateFn;\n  if (firstShardSpec instanceof OverwriteShardSpec) {\n    annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BuildingShardSpec) {\n    int segmentsPerInterval.size()=segmentsPerInterval.size();\n    int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n    if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n      LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n      throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n    }\n    annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n  }\n else   if (firstShardSpec instanceof BucketNumberedShardSpec) {\n    throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n  }\n else {\n    annotateFn=null;\n  }\n  if (annotateFn != null) {\n    intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n  }\n}\n",
      "nodeType" : "EnhancedForStatement",
      "astNodeNumber" : 212,
      "astHeight" : 15
    }, {
      "locationInParent" : "ChildProperty[org.eclipse.jdt.core.dom.MethodDeclaration,body]",
      "nodePosition" : {
        "charLength" : 2273,
        "startLineNumber" : 54,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "{\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int segmentsPerInterval.size()=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n      if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "Block",
      "astNodeNumber" : 270,
      "astHeight" : 16
    }, {
      "locationInParent" : "ChildListProperty[org.eclipse.jdt.core.dom.TypeDeclaration,bodyDeclarations]",
      "nodePosition" : {
        "charLength" : 2785,
        "startLineNumber" : 45,
        "startColumnNumber" : 2,
        "endLineNumber" : 113,
        "endColumnNumber" : 3
      },
      "nodeContext" : "/** \n * This method fills missing information in the shard spec if necessary when publishing segments. - When time chunk lock is used, the non-appending task should set the proper size of the core partitions for dynamically-partitioned segments. See  {@link #annotateCorePartitionSetSizeFn}. - When segment lock is used, the overwriting task should set the proper size of the atomic update group. See  {@link #annotateAtomicUpdateGroupFn}.\n */\nstatic Set<DataSegment> annotateShardSpec(Set<DataSegment> segments){\n  final Map<Interval,List<DataSegment>> intervalToSegments=new HashMap<>();\n  segments.forEach(segment -> intervalToSegments.computeIfAbsent(segment.getInterval(),k -> new ArrayList<>()).add(segment));\n  for (  Entry<Interval,List<DataSegment>> entry : intervalToSegments.entrySet()) {\n    final Interval interval=entry.getKey();\n    final List<DataSegment> segmentsPerInterval=entry.getValue();\n    final ShardSpec firstShardSpec=segmentsPerInterval.get(0).getShardSpec();\n    final boolean anyMismatch=segmentsPerInterval.stream().anyMatch(segment -> segment.getShardSpec().getClass() != firstShardSpec.getClass());\n    if (anyMismatch) {\n      throw new ISE(\"Mismatched shardSpecs in interval[%s] for segments[%s]\",interval,segmentsPerInterval);\n    }\n    final Function<DataSegment,DataSegment> annotateFn;\n    if (firstShardSpec instanceof OverwriteShardSpec) {\n      annotateFn=annotateAtomicUpdateGroupFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BuildingShardSpec) {\n      int segmentsPerInterval.size()=segmentsPerInterval.size();\n      int actualCorePartitionSetSize=Math.toIntExact(segmentsPerInterval.stream().filter(segment -> segment.getShardSpec().getPartitionNum() < segmentsPerInterval.size()).count());\n      if (segmentsPerInterval.size() != actualCorePartitionSetSize) {\n        LOG.errorSegments(segmentsPerInterval,\"Cannot publish segments due to incomplete time chunk\");\n        throw new ISE(\"Cannot publish segments due to incomplete time chunk for interval[%s]. \" + \"Expected [%s] segments in the core partition, but only [%] segments are found. \" + \"See task logs for more details about these segments.\",interval,segmentsPerInterval.size(),actualCorePartitionSetSize);\n      }\n      annotateFn=annotateCorePartitionSetSizeFn(segmentsPerInterval.size());\n    }\n else     if (firstShardSpec instanceof BucketNumberedShardSpec) {\n      throw new ISE(\"Cannot publish segments with shardSpec[%s]\",firstShardSpec);\n    }\n else {\n      annotateFn=null;\n    }\n    if (annotateFn != null) {\n      intervalToSegments.put(interval,segmentsPerInterval.stream().map(annotateFn).collect(Collectors.toList()));\n    }\n  }\n  return intervalToSegments.values().stream().flatMap(Collection::stream).collect(Collectors.toSet());\n}\n",
      "nodeType" : "MethodDeclaration",
      "astNodeNumber" : 286,
      "astHeight" : 17
    } ],
    "tokenLength" : 2,
    "type" : "int"
  } ],
  "positionList" : [ {
    "charLength" : 26,
    "startLineNumber" : 86,
    "startColumnNumber" : 78,
    "endLineNumber" : 86,
    "endColumnNumber" : 104
  }, {
    "charLength" : 26,
    "startLineNumber" : 89,
    "startColumnNumber" : 12,
    "endLineNumber" : 89,
    "endColumnNumber" : 38
  }, {
    "charLength" : 26,
    "startLineNumber" : 96,
    "startColumnNumber" : 14,
    "endLineNumber" : 96,
    "endColumnNumber" : 40
  }, {
    "charLength" : 26,
    "startLineNumber" : 100,
    "startColumnNumber" : 52,
    "endLineNumber" : 100,
    "endColumnNumber" : 78
  } ],
  "layoutRelationDataList" : [ ]
}